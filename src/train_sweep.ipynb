{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23313465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/bjcho/project/lightning-hydra-template')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "import lightning as L\n",
    "import rootutils\n",
    "import torch\n",
    "from lightning import Callback, LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import Logger\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4117f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg=compose(config_name=\"train.yaml\")\n",
    "\n",
    "\n",
    "# @hydra.main(version_base=\"1.3\", config_path=\"../configs\", config_name=\"train.yaml\")\n",
    "# def main(cfg: DictConfig) -> Optional[float]:\n",
    "#     \"\"\"Main entry point for training.\n",
    "\n",
    "#     :param cfg: DictConfig configuration composed by Hydra.\n",
    "#     :return: Optional[float] with optimized metric value.\n",
    "#     \"\"\"\n",
    "#     # apply extra utilities\n",
    "#     # (e.g. ask for tags if none are provided in cfg, print cfg tree, etc.)\n",
    "#     extras(cfg)\n",
    "\n",
    "#     # train the model\n",
    "#     metric_dict, _ = train(cfg)\n",
    "\n",
    "#     # safely retrieve metric value for hydra-based hyperparameter optimization\n",
    "#     metric_value = get_metric_value(\n",
    "#         metric_dict=metric_dict, metric_name=cfg.get(\"optimized_metric\")\n",
    "#     )\n",
    "\n",
    "#     # return optimized metric\n",
    "#     return metric_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14f61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cfc75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'train', 'tags': ['dev'], 'train': True, 'test': True, 'ckpt_path': None, 'seed': None, 'data': {'_target_': 'src.data.mnist_datamodule.MNISTDataModule', 'data_dir': '${paths.data_dir}', 'batch_size': 128, 'train_val_test_split': [55000, 5000, 10000], 'num_workers': 0, 'pin_memory': False}, 'model': {'_target_': 'src.models.mnist_module.MNISTLitModule', 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': 0.001, 'weight_decay': 0.0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ReduceLROnPlateau', '_partial_': True, 'mode': 'min', 'factor': 0.1, 'patience': 10}, 'net': {'_target_': 'src.models.components.simple_dense_net.SimpleDenseNet', 'input_size': 784, 'lin1_size': 64, 'lin2_size': 128, 'lin3_size': 64, 'output_size': 10}, 'compile': False}, 'callbacks': {'model_checkpoint': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'dirpath': '${paths.output_dir}/checkpoints', 'filename': 'epoch_{epoch:03d}', 'monitor': 'val/acc', 'verbose': False, 'save_last': True, 'save_top_k': 1, 'mode': 'max', 'auto_insert_metric_name': False, 'save_weights_only': False, 'every_n_train_steps': None, 'train_time_interval': None, 'every_n_epochs': None, 'save_on_train_epoch_end': None}, 'early_stopping': {'_target_': 'lightning.pytorch.callbacks.EarlyStopping', 'monitor': 'val/acc', 'min_delta': 0.0, 'patience': 100, 'verbose': False, 'mode': 'max', 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None}, 'model_summary': {'_target_': 'lightning.pytorch.callbacks.RichModelSummary', 'max_depth': -1}, 'rich_progress_bar': {'_target_': 'lightning.pytorch.callbacks.RichProgressBar'}}, 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'default_root_dir': '${paths.output_dir}', 'min_epochs': 1, 'max_epochs': 10, 'accelerator': 'cpu', 'devices': 1, 'check_val_every_n_epoch': 1, 'deterministic': False}, 'paths': {'root_dir': '${oc.env:PROJECT_ROOT}', 'data_dir': '${paths.root_dir}/data/', 'log_dir': '${paths.root_dir}/logs/', 'output_dir': '${hydra:runtime.output_dir}', 'work_dir': '${hydra:runtime.cwd}'}, 'extras': {'ignore_warnings': False, 'enforce_tags': True, 'print_config': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8626a8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "HydraConfig was not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhydra_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HydraConfig\n\u001b[0;32m----> 3\u001b[0m HydraConfig\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhydra\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hydra/core/hydra_config.py:31\u001b[0m, in \u001b[0;36mHydraConfig.get\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m instance \u001b[38;5;241m=\u001b[39m HydraConfig\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHydraConfig was not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhydra\n",
      "\u001b[0;31mValueError\u001b[0m: HydraConfig was not set"
     ]
    }
   ],
   "source": [
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "HydraConfig.get().overrides.hydra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
